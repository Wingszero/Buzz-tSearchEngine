# ISSUES & TODO
1. Query efficiency

2. Indexer performance

3. Page query location

4. More
Phrase indexing etc.


# How to launch an EC-2 cluster and deploy Indexer.

## launch and prepare the EC-2 cluster

Change '~/master.pem' to your key in launch_cluster.sh, login.sh, start.sh, stop.sh, update-java.sh. Move spark_ec2.py into your spark/ec2/ folder. These scripts can be found in indexer/scripts.

Run ./launch_cluster.sh

Wait for the cluster to launch.
In the future, You can login to master by ./login.sh, you can login to slave by 'ssh -i ~/your.pem root@ec2-SLAVE-HOSTNAME'. 

After the cluster has launched successfully, run the following.

./update-java.sh
./move-spark-workdir.sh
scp -i ~/your.pem fix_spark_ebs.sh root@ec2-MASTER-HOSTNAME:
scp -i ~/your.pem fix_spark_local.sh root@ec2-MASTER-HOSTNAME:
scp -i ~/your.pem mount.sh root@ec2-MASTER-HOSTNAME:
scp -i ~/your.pem run.sh root@ec2-MASTER-HOSTNAME:
scp -i ~/your.pem ranker.jar root@ec2-MASTER-HOSTNAME:
scp -i ~/your.pem spark.conf root@ec2-MASTER-HOSTNAME:
scp -i ~/your.pem extract.sh root@ec2-MASTER-HOSTNAME:/vol0/
scp -i ~/your.pem dbExtractor.jar root@ec2-MASTER-HOSTNAME:/vol0/
scp -i ~/your.pem parseWiki.py root@ec2-MASTER-HOSTNAME:/vol0/
scp -i ~/your.pem db.conf root@ec2-MASTER-HOSTNAME:/vol0/

Now login to master.

./login.sh
./fix_spark_ebs.sh
./fix_spark_local.sh

Right now the cluster should be ready, but we still lack the data for our ranker. Note that everytime after you stop.sh and start.sh the cluster, you need to login and do ./fix_spark_ebs.sh and ./fix_spark_local.sh. You also need to change the hostname of master in file spark.conf, as it has changed after restart.

## Prepare data

First we prepare data crawled by the crawler.
Go to the AWS console, in Volumes, select Create Volume, enter the Snapshot ID of, note that you must select the Availability Zone the same as your cluster's.
Now, right click the created volumes, select attach volume, and choose the master node. Then, we login to master and extract our data.

./login.sh
lsblk
mkdir /mnt/data1
mkdir /mnt/data2
mkdir /mnt/data3
./mount.sh
cd /vol0
./extract.sh

This would take a while, as the extractor would generate the file 'corpus'. After completion, we can do the following to put the corpus into our HDFS.

cd ~
./persistent-hdfs/bin/hadoop fs -mkdir -p /user/root
./persistent-hdfs/bin/hadoop fs -mkdir -p /testdir
./persistent-hdfs/bin/hadoop fs -put corpus /testdir/

Now we prepare data for our knowledge graph, remove the break in parseWiki.py.

cd /vol0
wget https://dumps.wikimedia.org/enwiki/20151201/enwiki-20151201-abstract.xml
python parseWiki.py

This would take a while, as the script would generate the file 'wiki'. After completion, we can do the following to put the data into our HDFS.

cd ~
./persistent-hdfs/bin/hadoop fs -put wiki /testdir/

Now the preparation for our data is complete.

## Run Ranker

cd ~
./run.sh

Wait for the ranker to run. You can view the status through your webbrowser in ec2-MASTER-HOSTNAME:8080.
If success, it should finish and stop to ask you to input your query.


# How to run unit test

Build and run RunAllTests.
Note that some tests(DbExtractor etc) will not work, as we have not submitted the test db with this submission(small test db with 100Mb).

# Dependencies
datanucleus-api-jdo-3.2.6.jar  
guava-18.0.jar         
jsoup-1.8.3.jar               
spark-assembly-1.5.1-hadoop2.6.0.jar
datanucleus-core-3.2.10.jar    
hamcrest-core-1.3.jar  
junit.jar                     
spark-examples-1.5.1-hadoop2.6.0.jar
datanucleus-rdbms-3.2.9.jar    
je-6.2.31.jar          
spark-1.5.1-yarn-shuffle.jar


